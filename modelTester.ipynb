{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP14.edf\n",
      "DP18.edf\n",
      "DP141.edf\n",
      "DP142.edf\n",
      "DP15.edf\n",
      "(369495, 64, 2)\n",
      "0.697108553566354\n"
     ]
    }
   ],
   "source": [
    "import pyedflib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "#Samples are represented in 16-bit 2's complement\n",
    "\n",
    "# Get the directory where the script is located\n",
    "script_dir = os.getcwd()\n",
    "file_counter = 0\n",
    "Files = []\n",
    "badFiles = []\n",
    "# Construct the full path to the file\n",
    "file_path = os.path.join(script_dir, 'EDF', 'PD patient Frontal')\n",
    "for filename in os.listdir(file_path):\n",
    "    # Check if the file ends with .edf\n",
    "    if filename.endswith('.edf'):\n",
    "        file_counter = file_counter+1\n",
    "        Files.append(filename)\n",
    "for k in np.arange(file_counter):\n",
    "    path = os.path.join(file_path, Files[k])\n",
    "    try:\n",
    "        f = pyedflib.EdfReader(path)\n",
    "    except OSError:\n",
    "        badFiles.append(Files[k])     \n",
    "Files = [item for item in Files if item not in badFiles]\n",
    "Files.reverse()\n",
    "n = f.signals_in_file\n",
    "n = n-9\n",
    "number_of_samples = f.getNSamples()[0]\n",
    "Nblocks = int((number_of_samples-250)/64)\n",
    "TotalBlocks=(5*Nblocks)*n\n",
    "fuller_data = np.ndarray(shape=(TotalBlocks, 64, 2))\n",
    "BlockCount=0\n",
    "multiplier = f.getPhysicalMaximum(0)/f.getDigitalMaximum(0)\n",
    "f.close()\n",
    "for index, name in enumerate(Files):\n",
    "    print(name)\n",
    "    path = os.path.join(file_path, name)\n",
    "    f = pyedflib.EdfReader(path)\n",
    "    number_of_samples = f.getNSamples()[0]\n",
    "    Nblocks = int((number_of_samples-250)/64)\n",
    "    sigbufs = np.zeros(number_of_samples)\n",
    "    full_data64 = np.ndarray(shape=(Nblocks*n, 64, 2))\n",
    "    signalList = []  \n",
    "    BlockCount = BlockCount+Nblocks*n\n",
    "    ran = np.ndarray(shape=(21, number_of_samples-250))\n",
    "    for i in np.arange(n):\n",
    "        sigbufs[:] = f.readSignal(i, digital=True)\n",
    "        sigbufs_new = sigbufs[250:]\n",
    "        #ran[i] = sigbufs_new\n",
    "        signalList.append(sigbufs_new) \n",
    "        labels = np.zeros(number_of_samples-250)\n",
    "        if name == \"DP14.edf\":\n",
    "            sezStart = 79650\n",
    "            sezEnd = 82250\n",
    "            labels[sezStart:sezEnd] = 1\n",
    "        elif name == \"DP141.edf\":\n",
    "            sezStart = 103500\n",
    "            sezEnd = 106500\n",
    "            labels[sezStart:sezEnd] = 1\n",
    "        elif name == \"DP142.edf\":\n",
    "            sezStart = 223250\n",
    "            sezEnd = 224750\n",
    "            labels[sezStart:sezEnd] = 1\n",
    "        elif name == \"DP18.edf\":\n",
    "            sezStart = 93250\n",
    "            sezEnd = 94000\n",
    "            labels[sezStart:sezEnd] = 1\n",
    "        for j in np.arange(Nblocks):\n",
    "                full_data64[i*Nblocks+j,:,0] = signalList[i][j*64:(j+1)*64]\n",
    "                full_data64[i*Nblocks+j,:,1] = labels[j*64:(j+1)*64]\n",
    "    fuller_data[BlockCount-Nblocks*n:BlockCount] = full_data64\n",
    "    f.close()\n",
    "seizures = np.sum(fuller_data[:,:,1] == 1) \n",
    "normal = np.sum(fuller_data[:,:,1] == 0)\n",
    "percentage_seizure = seizures/(seizures+normal)*100\n",
    "print(fuller_data.shape)\n",
    "print(percentage_seizure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 22:08:45.525691: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-13 22:08:45.525743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-13 22:08:45.526447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-13 22:08:45.530851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-13 22:08:46.256596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def prd_loss_dig2phy(y_true, y_pred):\n",
    "    y_true = (y_true)*multiplier\n",
    "    y_pred = (y_pred)*multiplier\n",
    "    rms_deviation = (tf.reduce_sum(tf.square(y_true - y_pred)))\n",
    "    percentage_rmsd = tf.sqrt(rms_deviation/(tf.reduce_sum(tf.square(y_true))+tf.keras.backend.epsilon()))* 100\n",
    "    return percentage_rmsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def prd_loss_dig2phy_new(y_true, y_pred):\n",
    "    y_true_processed = y_true[:, :, 0]\n",
    "    y_true_processed = y_true_processed[..., tf.newaxis]\n",
    "    y_true_processed = (y_true_processed)*multiplier\n",
    "    y_pred = (y_pred)*multiplier\n",
    "    rms_deviation = (tf.reduce_sum(tf.square(y_true_processed - y_pred)))\n",
    "    percentage_rmsd = tf.sqrt(rms_deviation/(tf.reduce_sum(tf.square(y_true_processed))+tf.keras.backend.epsilon()))* 100\n",
    "    return percentage_rmsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def weighted_mse_loss(y_true, y_pred):\n",
    "    # y_true, y_pred shape: (batch_size, num_channels, signal_length)\n",
    "    # label shape: (batch_size, )\n",
    "    weight = 200\n",
    "    # Extract the labels (0 or 1) from the last dimension of y_true\n",
    "    labels = y_true[:, :, 1]\n",
    "    # Remove the labels from y_true for loss calculation\n",
    "    y_true_processed = y_true[:, :, 0]\n",
    "    y_true_processed=y_true_processed[..., tf.newaxis]\n",
    "    loss = K.mean(K.square(y_pred - y_true_processed))\n",
    "    weighted_loss = loss * ((labels * (weight - 1)) + 1)\n",
    "    return K.mean(weighted_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weighted_mae_loss(y_true, y_pred):\n",
    "    # y_true, y_pred shape: (batch_size, signal_length, features)\n",
    "    weight = 100\n",
    "    # Extract the labels (0 or 1) from the last feature dimension of y_true\n",
    "    labels = y_true[:, :, 1]  # Assuming the label is the last feature\n",
    "    #labels = tf.expand_dims(labels, axis=-1)  # Expand dims to make labels shape (batch_size, signal_length, 1)\n",
    "    \n",
    "    # Remove the labels from y_true for loss calculation\n",
    "    y_true_processed = y_true[:, :, 0]  # Assuming the first feature is the target value\n",
    "    y_true_processed = y_true_processed[..., tf.newaxis]  # Expand dims to make shape (batch_size, signal_length, 1)\n",
    "    # Compute the absolute error\n",
    "    prd_error = prd_loss_dig2phy(y_true_processed, y_pred)\n",
    "\n",
    "    # Apply the weight based on the labels\n",
    "    weighted_abs_error = prd_error * (labels * (weight - 1) + 1)\n",
    "\n",
    "    # Compute the mean of the weighted absolute error\n",
    "    weighted_loss = tf.reduce_mean(weighted_abs_error)\n",
    "    return weighted_loss\n",
    "\n",
    "# Continue with the model definition and training as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 22:09:00.142894: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 22:09:00.170303: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 22:09:00.170623: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 22:09:00.174167: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 22:09:00.174526: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 22:09:00.174835: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 22:09:01.612558: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 22:09:01.612991: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 22:09:01.613007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-13 22:09:01.613279: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 22:09:01.613310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2073 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " autoencoder_input (InputLa  [(None, 64, 1)]           0         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 16)                1580      \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 64, 1)             2841      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4421 (17.27 KB)\n",
      "Trainable params: 4421 (17.27 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 22:09:05.037103: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-07-13 22:09:05.111723: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-07-13 22:09:05.873878: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-07-13 22:09:06.813233: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f039aba0b50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-13 22:09:06.813309: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650 Ti, Compute Capability 7.5\n",
      "2024-07-13 22:09:06.818715: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720904946.902687  109896 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4619/4619 [==============================] - ETA: 0s - loss: 58441.1836 - prd_loss_dig2phy_new: 48.5237\n",
      "Epoch 1: val_loss improved from inf to 30527.14453, saving model to sez_model.keras\n",
      "4619/4619 [==============================] - 62s 12ms/step - loss: 58441.1836 - prd_loss_dig2phy_new: 48.5237 - val_loss: 30527.1445 - val_prd_loss_dig2phy_new: 35.8742\n",
      "Epoch 2/30\n",
      "4618/4619 [============================>.] - ETA: 0s - loss: 25572.2324 - prd_loss_dig2phy_new: 33.9867\n",
      "Epoch 2: val_loss improved from 30527.14453 to 18515.79492, saving model to sez_model.keras\n",
      "4619/4619 [==============================] - 57s 12ms/step - loss: 25568.7559 - prd_loss_dig2phy_new: 33.9842 - val_loss: 18515.7949 - val_prd_loss_dig2phy_new: 28.6269\n",
      "Epoch 3/30\n",
      "4616/4619 [============================>.] - ETA: 0s - loss: 24040.4375 - prd_loss_dig2phy_new: 30.9867\n",
      "Epoch 3: val_loss did not improve from 18515.79492\n",
      "4619/4619 [==============================] - 58s 13ms/step - loss: 24026.9922 - prd_loss_dig2phy_new: 30.9919 - val_loss: 29000.9434 - val_prd_loss_dig2phy_new: 37.9102\n",
      "Epoch 4/30\n",
      "4614/4619 [============================>.] - ETA: 0s - loss: 20963.9531 - prd_loss_dig2phy_new: 28.8058\n",
      "Epoch 4: val_loss improved from 18515.79492 to 13607.65234, saving model to sez_model.keras\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 20954.2734 - prd_loss_dig2phy_new: 28.8048 - val_loss: 13607.6523 - val_prd_loss_dig2phy_new: 24.9528\n",
      "Epoch 5/30\n",
      "4616/4619 [============================>.] - ETA: 0s - loss: 24731.2383 - prd_loss_dig2phy_new: 29.7991\n",
      "Epoch 5: val_loss did not improve from 13607.65234\n",
      "4619/4619 [==============================] - 48s 10ms/step - loss: 24722.9180 - prd_loss_dig2phy_new: 29.7986 - val_loss: 15682.8418 - val_prd_loss_dig2phy_new: 27.7771\n",
      "Epoch 6/30\n",
      "4614/4619 [============================>.] - ETA: 0s - loss: 20201.2676 - prd_loss_dig2phy_new: 27.3568\n",
      "Epoch 6: val_loss improved from 13607.65234 to 10562.68359, saving model to sez_model.keras\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 20182.3008 - prd_loss_dig2phy_new: 27.3486 - val_loss: 10562.6836 - val_prd_loss_dig2phy_new: 21.3963\n",
      "Epoch 7/30\n",
      "4618/4619 [============================>.] - ETA: 0s - loss: 21090.4062 - prd_loss_dig2phy_new: 29.3913\n",
      "Epoch 7: val_loss did not improve from 10562.68359\n",
      "4619/4619 [==============================] - 48s 10ms/step - loss: 21087.2988 - prd_loss_dig2phy_new: 29.3906 - val_loss: 13756.2441 - val_prd_loss_dig2phy_new: 25.1402\n",
      "Epoch 8/30\n",
      "4614/4619 [============================>.] - ETA: 0s - loss: 17866.9355 - prd_loss_dig2phy_new: 26.3387\n",
      "Epoch 8: val_loss did not improve from 10562.68359\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 17850.0430 - prd_loss_dig2phy_new: 26.3308 - val_loss: 11802.9648 - val_prd_loss_dig2phy_new: 22.6320\n",
      "Epoch 9/30\n",
      "4613/4619 [============================>.] - ETA: 0s - loss: 15070.6973 - prd_loss_dig2phy_new: 25.6888\n",
      "Epoch 9: val_loss did not improve from 10562.68359\n",
      "4619/4619 [==============================] - 48s 10ms/step - loss: 15055.1377 - prd_loss_dig2phy_new: 25.6896 - val_loss: 12215.6631 - val_prd_loss_dig2phy_new: 22.8522\n",
      "Epoch 10/30\n",
      "4617/4619 [============================>.] - ETA: 0s - loss: 15445.5469 - prd_loss_dig2phy_new: 25.5399\n",
      "Epoch 10: val_loss did not improve from 10562.68359\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 15440.1807 - prd_loss_dig2phy_new: 25.5392 - val_loss: 11885.1406 - val_prd_loss_dig2phy_new: 22.4536\n",
      "Epoch 11/30\n",
      "4619/4619 [==============================] - ETA: 0s - loss: 14936.0869 - prd_loss_dig2phy_new: 23.6769\n",
      "Epoch 11: val_loss did not improve from 10562.68359\n",
      "4619/4619 [==============================] - 50s 11ms/step - loss: 14936.0869 - prd_loss_dig2phy_new: 23.6769 - val_loss: 11696.8291 - val_prd_loss_dig2phy_new: 22.1928\n",
      "Epoch 12/30\n",
      "4617/4619 [============================>.] - ETA: 0s - loss: 17637.8281 - prd_loss_dig2phy_new: 25.9168\n",
      "Epoch 12: val_loss improved from 10562.68359 to 10213.31836, saving model to sez_model.keras\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 17631.4453 - prd_loss_dig2phy_new: 25.9147 - val_loss: 10213.3184 - val_prd_loss_dig2phy_new: 21.3236\n",
      "Epoch 13/30\n",
      "4618/4619 [============================>.] - ETA: 0s - loss: 18569.9961 - prd_loss_dig2phy_new: 25.0508\n",
      "Epoch 13: val_loss did not improve from 10213.31836\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 18567.3672 - prd_loss_dig2phy_new: 25.0514 - val_loss: 12208.7754 - val_prd_loss_dig2phy_new: 22.9952\n",
      "Epoch 14/30\n",
      "4614/4619 [============================>.] - ETA: 0s - loss: 17066.3555 - prd_loss_dig2phy_new: 25.0520\n",
      "Epoch 14: val_loss did not improve from 10213.31836\n",
      "4619/4619 [==============================] - 48s 10ms/step - loss: 17056.0879 - prd_loss_dig2phy_new: 25.0487 - val_loss: 12116.1377 - val_prd_loss_dig2phy_new: 22.8818\n",
      "Epoch 15/30\n",
      "4614/4619 [============================>.] - ETA: 0s - loss: 20782.9824 - prd_loss_dig2phy_new: 27.9340\n",
      "Epoch 15: val_loss did not improve from 10213.31836\n",
      "4619/4619 [==============================] - 50s 11ms/step - loss: 20882.8711 - prd_loss_dig2phy_new: 27.9335 - val_loss: 13690.7920 - val_prd_loss_dig2phy_new: 25.5358\n",
      "Epoch 16/30\n",
      "4617/4619 [============================>.] - ETA: 0s - loss: 13800.3232 - prd_loss_dig2phy_new: 23.4405\n",
      "Epoch 16: val_loss did not improve from 10213.31836\n",
      "4619/4619 [==============================] - 48s 10ms/step - loss: 13797.2041 - prd_loss_dig2phy_new: 23.4483 - val_loss: 18573.4453 - val_prd_loss_dig2phy_new: 28.4108\n",
      "Epoch 17/30\n",
      "4618/4619 [============================>.] - ETA: 0s - loss: 16178.3418 - prd_loss_dig2phy_new: 24.5135\n",
      "Epoch 17: val_loss did not improve from 10213.31836\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 16176.5439 - prd_loss_dig2phy_new: 24.5135 - val_loss: 12752.0508 - val_prd_loss_dig2phy_new: 23.3048\n",
      "Epoch 18/30\n",
      "4616/4619 [============================>.] - ETA: 0s - loss: 11321.1064 - prd_loss_dig2phy_new: 22.3487\n",
      "Epoch 18: val_loss improved from 10213.31836 to 10094.56934, saving model to sez_model.keras\n",
      "4619/4619 [==============================] - 48s 10ms/step - loss: 11314.8428 - prd_loss_dig2phy_new: 22.3441 - val_loss: 10094.5693 - val_prd_loss_dig2phy_new: 20.7064\n",
      "Epoch 19/30\n",
      "4616/4619 [============================>.] - ETA: 0s - loss: 15536.0654 - prd_loss_dig2phy_new: 23.9060\n",
      "Epoch 19: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 15528.3633 - prd_loss_dig2phy_new: 23.9105 - val_loss: 13603.9463 - val_prd_loss_dig2phy_new: 25.0113\n",
      "Epoch 20/30\n",
      "4617/4619 [============================>.] - ETA: 0s - loss: 17209.6777 - prd_loss_dig2phy_new: 24.6718\n",
      "Epoch 20: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 48s 10ms/step - loss: 17203.7051 - prd_loss_dig2phy_new: 24.6694 - val_loss: 12501.2451 - val_prd_loss_dig2phy_new: 23.9765\n",
      "Epoch 21/30\n",
      "4618/4619 [============================>.] - ETA: 0s - loss: 17546.8164 - prd_loss_dig2phy_new: 24.3735\n",
      "Epoch 21: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 17544.2910 - prd_loss_dig2phy_new: 24.3739 - val_loss: 16363.9316 - val_prd_loss_dig2phy_new: 30.4255\n",
      "Epoch 22/30\n",
      "4616/4619 [============================>.] - ETA: 0s - loss: 17447.0703 - prd_loss_dig2phy_new: 25.4457\n",
      "Epoch 22: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 17439.3164 - prd_loss_dig2phy_new: 25.4908 - val_loss: 119735.8906 - val_prd_loss_dig2phy_new: 92.3719\n",
      "Epoch 23/30\n",
      "4615/4619 [============================>.] - ETA: 0s - loss: 15107.4775 - prd_loss_dig2phy_new: 24.1944\n",
      "Epoch 23: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 48s 10ms/step - loss: 15095.6875 - prd_loss_dig2phy_new: 24.1871 - val_loss: 11030.3350 - val_prd_loss_dig2phy_new: 21.3582\n",
      "Epoch 24/30\n",
      "4613/4619 [============================>.] - ETA: 0s - loss: 16422.3457 - prd_loss_dig2phy_new: 24.7887\n",
      "Epoch 24: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 16430.6641 - prd_loss_dig2phy_new: 24.7826 - val_loss: 11660.5254 - val_prd_loss_dig2phy_new: 22.5754\n",
      "Epoch 25/30\n",
      "4616/4619 [============================>.] - ETA: 0s - loss: 12805.0322 - prd_loss_dig2phy_new: 22.5592\n",
      "Epoch 25: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 50s 11ms/step - loss: 12927.5850 - prd_loss_dig2phy_new: 22.5627 - val_loss: 11780.9961 - val_prd_loss_dig2phy_new: 21.6113\n",
      "Epoch 26/30\n",
      "4618/4619 [============================>.] - ETA: 0s - loss: 15258.0596 - prd_loss_dig2phy_new: 22.8188\n",
      "Epoch 26: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 15255.8418 - prd_loss_dig2phy_new: 22.8178 - val_loss: 11250.5273 - val_prd_loss_dig2phy_new: 22.3445\n",
      "Epoch 27/30\n",
      "4614/4619 [============================>.] - ETA: 0s - loss: 14146.2969 - prd_loss_dig2phy_new: 22.3587\n",
      "Epoch 27: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 14136.5752 - prd_loss_dig2phy_new: 22.3674 - val_loss: 22599.0000 - val_prd_loss_dig2phy_new: 36.0794\n",
      "Epoch 28/30\n",
      "4618/4619 [============================>.] - ETA: 0s - loss: 18287.0566 - prd_loss_dig2phy_new: 25.1274\n",
      "Epoch 28: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 18284.3535 - prd_loss_dig2phy_new: 25.1253 - val_loss: 10574.7441 - val_prd_loss_dig2phy_new: 21.0023\n",
      "Epoch 29/30\n",
      "4615/4619 [============================>.] - ETA: 0s - loss: 14099.6885 - prd_loss_dig2phy_new: 22.9115\n",
      "Epoch 29: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 48s 10ms/step - loss: 14088.7422 - prd_loss_dig2phy_new: 22.9061 - val_loss: 11015.9795 - val_prd_loss_dig2phy_new: 21.3420\n",
      "Epoch 30/30\n",
      "4619/4619 [==============================] - ETA: 0s - loss: 12482.5117 - prd_loss_dig2phy_new: 22.1719\n",
      "Epoch 30: val_loss did not improve from 10094.56934\n",
      "4619/4619 [==============================] - 49s 11ms/step - loss: 12482.5117 - prd_loss_dig2phy_new: 22.1719 - val_loss: 11878.3428 - val_prd_loss_dig2phy_new: 23.4558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f070cc00220>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the encoder\n",
    "input_ts = keras.layers.Input(shape=(64, 1), name=\"input_time_series\") # 64, 1\n",
    "x = keras.layers.Conv1D(16, 3, activation='linear', padding='same')(input_ts) # 64, 16\n",
    "x = keras.layers.Conv1D(8, 5, activation='linear', padding='same')(input_ts) # 64, 16\n",
    "x = keras.layers.Conv1D(8, 5, activation='linear', padding='same', strides=2)(x) # 32, 8\n",
    "x = keras.layers.Conv1D(4, 5, activation='linear', padding='same', strides=2)(x) # 16, 4\n",
    "x = keras.layers.Flatten()(x) # Flatten for Dense layer\n",
    "encoded = keras.layers.Dense(16, activation='linear')(x) # 16\n",
    "encoder = keras.models.Model(input_ts, encoded, name=\"encoder\")\n",
    "\n",
    "# Define the decoder\n",
    "encoded_input = keras.layers.Input(shape=(16,), name=\"encoded_input\") # 16\n",
    "x = keras.layers.Dense(16 * 4, activation='linear')(encoded_input) # 16 * 4\n",
    "x = keras.layers.Reshape((16, 4))(x) # Reshape back to (16, 4)\n",
    "x = keras.layers.Conv1DTranspose(8, 5, activation='linear', strides=2, padding='same')(x) # 16, 8\n",
    "x = keras.layers.Conv1DTranspose(16, 5, activation='linear', strides=2, padding='same')(x) # 32, 16\n",
    "x = keras.layers.Conv1DTranspose(8, 7, activation='linear', padding='same')(x) # 32, 16\n",
    "decoded = keras.layers.Conv1DTranspose(1, 3, activation='linear', padding='same')(x) # 64, 1\n",
    "\n",
    "decoder = keras.models.Model(encoded_input, decoded, name=\"decoder\")\n",
    "\n",
    "# Define the autoencoder\n",
    "autoencoder_input = keras.layers.Input(shape=(64, 1), name=\"autoencoder_input\")\n",
    "encoded_ts = encoder(autoencoder_input)\n",
    "decoded_ts = decoder(encoded_ts)\n",
    "\n",
    "autoencoder = keras.models.Model(autoencoder_input, decoded_ts, name=\"autoencoder\")\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss=weighted_mse_loss, metrics=[prd_loss_dig2phy_new])\n",
    "\n",
    "# Summary of the autoencoder\n",
    "autoencoder.summary()\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='sez_model.keras',  # Path to save the model\n",
    "    monitor='val_loss',        # Metric to monitor\n",
    "    save_best_only=True,       # Save only the best model\n",
    "    mode='min',                # Mode: minimize the monitored metric\n",
    "    verbose=1                  # Print a message when saving the model\n",
    ")\n",
    "#features_train = fuller_data[:, :, 0]\n",
    "# features_val = val_data[:, :, 0]\n",
    "\n",
    "new_train, new_val = train_test_split(fuller_data, test_size=0.2, random_state=42)\n",
    "features_val = new_val[:, :, 0]\n",
    "features_val = features_val[..., tf.newaxis]\n",
    "features_train = new_train[:, :, 0]\n",
    "features_train = features_train[..., tf.newaxis]\n",
    "#features_val = features_val[..., tf.newaxis]\n",
    "#print(features_train.shape)\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(features_train, new_train, epochs=30, batch_size=64, shuffle=True, callbacks=[checkpoint_callback], validation_data=(features_val, new_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62748, 64, 2)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "path = os.path.join(script_dir, 'EDF', 'DP153_new.edf')\n",
    "f = pyedflib.EdfReader(path)\n",
    "number_of_samples = f.getNSamples()[0]\n",
    "Nblocks = int((number_of_samples-250)/64)\n",
    "sigbufs = np.zeros(number_of_samples)\n",
    "full_data64 = np.ndarray(shape=(Nblocks*n, 64, 2))\n",
    "signalList = []  \n",
    "BlockCount = BlockCount+Nblocks*n\n",
    "ran = np.ndarray(shape=(21, number_of_samples-250))\n",
    "for i in np.arange(n):\n",
    "    sigbufs[:] = f.readSignal(i, digital=True)\n",
    "    sigbufs_new = sigbufs[250:]\n",
    "    #ran[i] = sigbufs_new\n",
    "    signalList.append(sigbufs_new) \n",
    "    for j in np.arange(Nblocks):\n",
    "        full_data64[i*Nblocks+j,:,0] = signalList[i][j*64:(j+1)*64]\n",
    "print(full_data64.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2310/2310 [==============================] - 7s 3ms/step\n",
      "13.53920549993215\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "best_autoencoder = keras.models.load_model('sez_model.keras', custom_objects={'weighted_mse_loss': weighted_mse_loss, 'prd_loss_dig2phy_new': prd_loss_dig2phy_new})\n",
    "test_data = full_data64[:, :,0][..., tf.newaxis]\n",
    "pred_data = best_autoencoder.predict(test_data)\n",
    "prd = prd_loss_dig2phy(test_data, pred_data).numpy()\n",
    "print(prd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qkeras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
